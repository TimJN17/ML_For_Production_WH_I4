{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "69093afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭───────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"> C:\\Users\\timjn\\AppData\\Local\\Temp/ipykernel_30440/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">578767199.py</span><span style=\"font-weight: bold\">:</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold\">8</span>                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>\"@misc<span style=\"font-weight: bold\">{</span>catalyst,                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     author = <span style=\"font-weight: bold\">{</span>Kolesnikov, Sergey<span style=\"font-weight: bold\">}</span>,                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     title = <span style=\"font-weight: bold\">{</span>Catalyst - Accelerated deep learning R&amp;D<span style=\"font-weight: bold\">}</span>,                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     year = <span style=\"font-weight: bold\">{</span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">2018</span><span style=\"font-weight: bold\">}</span>,                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     publisher = <span style=\"font-weight: bold\">{</span>GitHub<span style=\"font-weight: bold\">}</span>,                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     journal = <span style=\"font-weight: bold\">{</span>GitHub repository<span style=\"font-weight: bold\">}</span>,                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     howpublished = <span style=\"font-weight: bold\">{</span>\\url<span style=\"font-weight: bold\">{</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/catalyst-team/catalyst</span><span style=\"font-weight: bold\">}}</span>,                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"font-weight: bold\">}</span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>\"                                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">▲</span>                                                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span><span style=\"font-weight: bold\">(</span>unicode error<span style=\"font-weight: bold\">)</span> <span style=\"color: #008000; text-decoration-color: #008000\">'unicodeescape'</span> codec can't decode bytes in position <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">207</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">208</span>: \n",
       "truncated \\uXXXX escape\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭───────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m C:\\Users\\timjn\\AppData\\Local\\Temp/ipykernel_30440/\u001b[0m\u001b[1;33m578767199.py\u001b[0m\u001b[1m:\u001b[0m\u001b[1;94m8\u001b[0m                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m\"\"\u001b[0m\"@misc\u001b[1m{\u001b[0mcatalyst,                                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     author = \u001b[1m{\u001b[0mKolesnikov, Sergey\u001b[1m}\u001b[0m,                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     title = \u001b[1m{\u001b[0mCatalyst - Accelerated deep learning R&D\u001b[1m}\u001b[0m,                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     year = \u001b[1m{\u001b[0m\u001b[94m2018\u001b[0m\u001b[1m}\u001b[0m,                                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     publisher = \u001b[1m{\u001b[0mGitHub\u001b[1m}\u001b[0m,                                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     journal = \u001b[1m{\u001b[0mGitHub repository\u001b[1m}\u001b[0m,                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     howpublished = \u001b[1m{\u001b[0m\\url\u001b[1m{\u001b[0m\u001b[4;94mhttps://github.com/catalyst-team/catalyst\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1m}\u001b[0m\u001b[33m\"\"\u001b[0m\"                                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m     \u001b[1;91m▲\u001b[0m                                                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSyntaxError: \u001b[0m\u001b[1m(\u001b[0municode error\u001b[1m)\u001b[0m \u001b[32m'unicodeescape'\u001b[0m codec can't decode bytes in position \u001b[1;36m207\u001b[0m-\u001b[1;36m208\u001b[0m: \n",
       "truncated \\uXXXX escape\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"@misc{catalyst,\n",
    "    author = {Kolesnikov, Sergey},\n",
    "    title = {Catalyst - Accelerated deep learning R&D},\n",
    "    year = {2018},\n",
    "    publisher = {GitHub},\n",
    "    journal = {GitHub repository},\n",
    "    howpublished = {\\url{https://github.com/catalyst-team/catalyst}},\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1e74fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84cc150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "from json import dumps, loads\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ssh -o ServerAliveInterval=60 -L 9092:localhost:9092 tunnel@128.2.24.106 -NTf\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Update this for your demo otherwise you'll see my data :)\n",
    "topic = 'movielog4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df183e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Kafka Broker\n"
     ]
    }
   ],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    topic,\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    # Read from the start of the topic; Default is latest\n",
    "    auto_offset_reset='earliest',\n",
    "    # auto_offset_reset='latest',\n",
    "    # group_id='team13',\n",
    "    # Commit that an offset has been read\n",
    "    enable_auto_commit=True,\n",
    "    # How often to tell Kafka, an offset has been read\n",
    "    auto_commit_interval_ms=1000\n",
    ")\n",
    "\n",
    "index = 0\n",
    "\n",
    "print('Reading Kafka Broker...')\n",
    "for message in consumer:\n",
    "\n",
    "    message = message.value.decode()\n",
    "    # Default message.value type is bytes!\n",
    "    \n",
    "    # print(message)\n",
    "    \n",
    "#     if \"recommendation request 17645-team04\" in message:\n",
    "#         print(message)\n",
    "    \n",
    "    if \"/rate/\" in message:\n",
    "#         print(message)\n",
    "        os.system(f\"echo {message} >> kafka_online_ratings_1APRIL23_log.csv\")\n",
    "        index += 1\n",
    "        if index >= 1000:\n",
    "            print(\"Breaking loop.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "05ebe8e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   DTG  UserID                                     ratings\n",
      "0  2023-03-28T15:04:43  415014       GET /rate/simon+of+the+desert+1965=4 \n",
      "1  2023-03-28T15:04:44  815048        GET /rate/the+wrong+trousers+1993=4 \n",
      "2  2023-03-28T15:04:45  745126                 GET /rate/alexander+2004=1 \n",
      "3  2023-03-28T15:04:46  204326             GET /rate/the+godfather+1972=5 \n",
      "4  2023-03-28T15:04:46  370012  GET /rate/the+shawshank+redemption+1994=5 \n"
     ]
    }
   ],
   "source": [
    "\"\"\" READING DATA \"\"\"\n",
    "\n",
    "# Define column names\n",
    "col_names = [\"DTG\", \"UserID\", \"ratings\"]\n",
    "\n",
    "# Change to the appropriate directory\n",
    "os.chdir(\"C:/Users/timjn/OneDrive/Documents/Class, 11695 ML For Production/Assignment_4\")\n",
    "\n",
    "# Read the acquired data using Pandas\n",
    "vanilla_data = pd.read_csv(\"kafka_online_ratings_1APRIL23_log.csv\", names=col_names, header=None)\n",
    "\n",
    "# Check our data\n",
    "print(vanilla_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "15bfea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID ratings                     Movie_Name  Movie_ID\n",
      "0  415014      4        simon+of+the+desert+1965       0.0\n",
      "1  815048      4         the+wrong+trousers+1993       1.0\n",
      "2  745126      1                  alexander+2004       2.0\n",
      "3  204326      5              the+godfather+1972       3.0\n",
      "4  370012      5   the+shawshank+redemption+1994       4.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data for the mvie name\n",
    "movies = vanilla_data[\"ratings\"].apply(lambda x: x.split(\"/rate/\")[1].split(\"=\")[0])\n",
    "\n",
    "# get unique movie names\n",
    "unique_movies = movies.unique()\n",
    "\n",
    "# create new dataframe with unique movie IDs\n",
    "movie_ID_dataframe = pd.DataFrame({'Movie_Name': unique_movies,\n",
    "                       'Movie_ID': np.arange(len(unique_movies))})\n",
    "\n",
    "# Append the movie data to the vanilla dataframe\n",
    "vanilla_data = pd.concat([vanilla_data, movie_ID_dataframe], axis=1)\n",
    "    \n",
    "# Split the ratings line for the numer\n",
    "vanilla_data[\"ratings\"] = vanilla_data[\"ratings\"].apply(lambda x: x.split(\"=\")[1])\n",
    "\n",
    "# Drop bad data; i.e. a letter being used for a rating [A, F, C, ...] in place a 1-5 numerical rating\n",
    "vanilla_data[\"ratings\"] = vanilla_data[\"ratings\"][pd.to_numeric(vanilla_data[\"ratings\"], errors='coerce').notnull()]\n",
    "\n",
    "# Drop the Date Time Group \"DTG\" column\n",
    "vanilla_data = vanilla_data.drop(\"DTG\", axis=1)\n",
    "\n",
    "# Check our data\n",
    "print(vanilla_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6abc7798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create the Sparse Matrix from the Pandas Dataframe\n",
    "def load_review_data(dataframe: pd.DataFrame):\n",
    "    \"\"\"load_review_data\n",
    "    Load movie review data.\n",
    "    Input has format MovieID1,UserID11,rating_score_for_UserID11_to_MovieID1.\n",
    "\n",
    "    :param path:\n",
    "    returns final _rray\n",
    "    \"\"\"\n",
    "\n",
    "    user_count = max(dataframe['UserID'].unique())\n",
    "    movie_count = max(dataframe['Movie_ID'].unique())\n",
    "    \n",
    "    final_array = np.zeros((int(user_count),int(movie_count)))\n",
    "    \n",
    "    data, row, col = [], [], []\n",
    "    for row in dataframe.iterrows():\n",
    "        \n",
    "        user_index = int(row[1].iloc[0])\n",
    "        score = float(row[1].iloc[1])\n",
    "        try:\n",
    "            movie_index = int(row[1].iloc[3])\n",
    "            \n",
    "        except ValueError:  # Used to navigate an error with movieID 345\n",
    "            final_array[user_index-1, movie_index-1] = score    \n",
    "                \n",
    "        final_array[user_index-1, movie_index-1] = score   \n",
    "\n",
    "    return final_array\n",
    "\n",
    "# Define the object to be sent into the Catalyst functions\n",
    "data_for_Catalyst = load_review_data(vanilla_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4288978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IMPLEMENTING CATALYST RECOMMENDATION SYSTEM \"\"\"\n",
    "\n",
    "# prepare data\n",
    "num_items = data_for_Catalyst.shape[1]  # --> my data set has this numer as 345\n",
    "num_users = data_for_Catalyst.shape[0]  # --> my data set has this numer as 947686\n",
    "num_features = data_for_Catalyst.shape[1]\n",
    "\n",
    "# Define usable tensor objects\n",
    "X = torch.tensor(data_for_Catalyst).float()  # --> torch.Size([num_users, num_features])\n",
    "y = (torch.rand(num_users, num_items) > 0.5).to(torch.float32)  # --> torch.Size([numn_users, num_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b5c3be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_items)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a554d4e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f277fca3eb745dcaddb0ecec63b76f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/3 * Epoch (train):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/3) loss: 0.6931734325195709 | loss/mean: 0.6931734325195709 | loss/std: 7.324247938279405e-05 | lr: 0.001 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14af56618aeb479e9b0cb8552dbdc6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/3 * Epoch (valid):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/3) loss: 0.69314670917849 | loss/mean: 0.69314670917849 | loss/std: 0.0002584077909227108 | lr: 0.001 | momentum: 0.9\n",
      "* Epoch (1/3) lr: 0.001 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134ac9448f0f49cf9d589cfaf844bce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2/3 * Epoch (train):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2/3) loss: 0.6931509151532923 | loss/mean: 0.6931509151532923 | loss/std: 0.0002578164998813689 | lr: 0.001 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324945a421bc40f2a28079baa6197f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2/3 * Epoch (valid):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (2/3) loss: 0.6931267836700175 | loss/mean: 0.6931267836700175 | loss/std: 0.0004866400981969976 | lr: 0.001 | momentum: 0.9\n",
      "* Epoch (2/3) lr: 0.0001 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a480c56bd53a41f2985b4f98c7748741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3/3 * Epoch (train):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3/3) loss: 0.693110140502279 | loss/mean: 0.693110140502279 | loss/std: 0.00048415166084610694 | lr: 0.0001 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7162a722e4ca4376af85899c6ef449e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3/3 * Epoch (valid):   0%|          | 0/29616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (3/3) loss: 0.6931051888136126 | loss/mean: 0.6931051888136126 | loss/std: 0.0005066142662548302 | lr: 0.0001 | momentum: 0.9\n",
      "* Epoch (3/3) lr: 0.0001 | momentum: 0.9\n",
      "Top models:\n",
      "./logs/model.0003.pth\t0.6931\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    num_epochs=3,\n",
    "    verbose=True,\n",
    "    callbacks=[\n",
    "        dl.BatchTransformCallback(\n",
    "            transform=torch.sigmoid,\n",
    "            scope=\"on_batch_end\",\n",
    "            input_key=\"logits\",\n",
    "            output_key=\"scores\"\n",
    "        ),\n",
    "        dl.CriterionCallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"),\n",
    "        dl.BackwardCallback(metric_key=\"loss\"),\n",
    "        dl.OptimizerCallback(metric_key=\"loss\"),\n",
    "        dl.SchedulerCallback(),\n",
    "        dl.CheckpointCallback(\n",
    "            logdir=\"./logs\", loader_key=\"valid\", metric_key=\"loss\", minimize=True\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af0a0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use any unique User ID to get preditions for that user\n",
    "arbitrary_user_ID = 20345\n",
    "\n",
    " # Need to ensure the tensor passed into the runner model is 2D\n",
    "user_information = torch.unsqueeze(X[arbitrary_user_ID, :], 0)    # --> will be torch.Size([1, ###])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e2859096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(345,)\n"
     ]
    }
   ],
   "source": [
    "# Produce the predictions for items for the input user ID\n",
    "preds = runner.model(user_information).detach().numpy().flatten()\n",
    "print(preds.shape)  # --> will be an array of shape == [num_items, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e7f3aa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24 306  36 229  60 318 308 256 331  25 102 209 245  62 243 132  18  90\n",
      " 184 142]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the top recommended items with 20 == number recommended items\n",
    "top_indices = np.argsort(preds)[::-1][:20]\n",
    "\n",
    "# Map the indices back to the original item names using the combined movie_ID_dataframe object\n",
    "top_items = [movie_ID_dataframe[\"Movie_Name\"][i] for i in top_indices]\n",
    "\n",
    "# Print them to ensure we have reasonable indices\n",
    "print(top_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "408dd029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Movie_Name  Ranking_Position\n",
      "0                            the+usual+suspects+1995                 1\n",
      "1                                   point+blank+2010                 2\n",
      "2                             l.a.+confidential+1997                 3\n",
      "3                               men+in+black+ii+2002                 4\n",
      "4                                anchors+aweigh+1945                 5\n",
      "5                        terror+beneath+the+sea+1966                 6\n",
      "6                        love+valour+compassion+1997                 7\n",
      "7                                        porkys+1981                 8\n",
      "8                                excess+baggage+1997                 9\n",
      "9   harry+potter+and+the+deathly+hallows+part+1+2010                10\n",
      "10                                    twin+town+1997                11\n",
      "11                         bloody+pit+of+horror+1965                12\n",
      "12                                   fight+club+1999                13\n",
      "13                                   metropolis+1927                14\n",
      "14              white+shadows+in+the+south+seas+1928                15\n",
      "15                                         scum+1979                16\n",
      "16                                      vertigo+1958                17\n",
      "17                   the+women+on+the+6th+floor+2010                18\n",
      "18                               cool+hand+luke+1967                19\n",
      "19           national+theatre+live+frankenstein+2011                20\n"
     ]
    }
   ],
   "source": [
    "# Print the top items\n",
    "recommendation_df = pd.DataFrame({'Movie_Name': top_items, 'Ranking_Position': range(1, len(top_items)+1)})\n",
    "print(recommendation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd25ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, movie in enumerate(top_items):\n",
    "    \n",
    "#     print(f\"The {index} recommended movie for user_ID << {arbitrary_user_ID} >> is movie_ID: << {movie} >> .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
